{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "K5dn1qlTu96Z",
        "AXDgFaz5vEuY",
        "cbx-7WCXxBKw"
      ],
      "authorship_tag": "ABX9TyPBVVVFTQMlJEaxknF0n48W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GenAIUnplugged/langchain_series/blob/main/03_Chains.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mObGdtUbtBZP",
        "outputId": "3a586d80-9d95-4136-8968-e7f8f7821da9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.16-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.24)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.11/dist-packages (0.3.56)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.23-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting langchain-core\n",
            "  Downloading langchain_core-0.3.59-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.76.2)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.39)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.4)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (4.13.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_openai-0.3.16-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.59-py3-none-any.whl (437 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.7/437.7 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.23-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, tiktoken, pydantic-settings, dataclasses-json, langchain-core, langchain-openai, langchain-community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.56\n",
            "    Uninstalling langchain-core-0.3.56:\n",
            "      Successfully uninstalled langchain-core-0.3.56\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.23 langchain-core-0.3.59 langchain-openai-0.3.16 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.9.1 python-dotenv-1.1.0 tiktoken-0.9.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-openai langchain langchain-core langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "H2lWC2KAtIIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "model = ChatOpenAI(temperature=0,model=\"gpt-4o-mini\")"
      ],
      "metadata": {
        "id": "4Qe6Y0Q6tLko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLMChain"
      ],
      "metadata": {
        "id": "K5dn1qlTu96Z"
      }
    },
    {
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# Define a prompt template\n",
        "template = \"\"\"You are a helpful assistant.\n",
        "Answer the following question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# Create the LLMChain\n",
        "# We're assuming 'model' (a ChatOpenAI instance) has been defined previously\n",
        "chain = LLMChain(prompt=prompt, llm=model)\n",
        "\n",
        "# Invoke the chain with a question\n",
        "question_text = \"What is the capital of Bulgaria?\"\n",
        "response = chain.invoke({\"question\":question_text})\n",
        "\n",
        "# Print the response\n",
        "print(response)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZOdt3HTt-8Y",
        "outputId": "adbdb909-98f3-49da-bf4b-d0da64266657"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'question': 'What is the capital of Bulgaria?', 'text': 'The capital of Bulgaria is Sofia.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SimpleSequentialChain"
      ],
      "metadata": {
        "id": "AXDgFaz5vEuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SimpleSequentialChain\n",
        "\n",
        "# Define the first chain (we already have `chain` from the previous example)\n",
        "\n",
        "# Define a second prompt template and chain\n",
        "template2 = \"\"\"Given the capital city of a country, tell me something interesting about that city.\n",
        "Capital city: {capital}\n",
        "\"\"\"\n",
        "prompt2 = ChatPromptTemplate.from_template(template2)\n",
        "chain2 = LLMChain(prompt=prompt2, llm=model) # Use the same model\n",
        "\n",
        "# Create the SimpleSequentialChain\n",
        "# The output of the first chain becomes the input of the second chain\n",
        "# The input variable names must match the output variable names\n",
        "# In this case, the output of the first chain's default is 'text', which matches the input of the second chain's 'capital' if named 'text'\n",
        "# However, if we want to explicitly map, we can define input_variables in chain2 to match chain's output.\n",
        "# For SimpleSequentialChain, the output of the first chain is automatically passed as the single input to the second.\n",
        "# We need to ensure the output key of the first chain matches the *single* input key of the second chain, or rely on the default.\n",
        "# The default output key for an LLMChain is 'text'. So, if the second chain has a single input variable (like 'capital'),\n",
        "# SimpleSequentialChain will automatically map the first chain's 'text' output to the second chain's 'capital' input.\n",
        "\n",
        "sequential_chain = SimpleSequentialChain(chains=[chain, chain2], verbose=True)\n",
        "\n",
        "# Run the sequential chain\n",
        "# The input to the sequential chain is the input to the first chain ('question')\n",
        "input_data = {\"question\": \"What is the capital of Bulgaria?\"}\n",
        "response = sequential_chain.invoke(input_data)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "pJqyGKqgvDWg",
        "outputId": "836240a2-8823-4433-d8b2-10ef4b2065fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Missing some input keys: {'input'}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-6d8e64363343>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# The input to the sequential chain is the input to the first chain ('question')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0minput_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"question\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"What is the capital of Bulgaria?\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequential_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m         )\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m             outputs = (\n\u001b[1;32m    157\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m_validate_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0mmissing_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdifference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmissing_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Missing some input keys: {missing_keys}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Missing some input keys: {'input'}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why we are getting error ?**\\\n",
        "Looking at the definition of the SimpleSequentialChain, it takes the output of the first chain and passes it as the single input to the second chain. By default, the LLMChain outputs its result under the key 'text'.\n",
        "\n",
        "In this case, the first chain (chain) has an input key 'question'. The second chain (chain2) has an input key 'capital'. SimpleSequentialChain tries to map the output of the first chain to the input of the second chain. However, the error suggests that the SimpleSequentialChain itself is looking for a top-level input variable named 'input' before even running the first chain.\n",
        "\n",
        "This is likely happening because SimpleSequentialChain expects the input it receives to be a single value (which it then passes to the first chain) or a dictionary with a single key named 'input' by default, unless the input key is explicitly defined. The current input {\"question\": \"What is the capital of Bulgaria?\"} does not match this expectation.\n",
        "\n",
        "**Solution:**\\\n",
        "To resolve this, we need to ensure the input dictionary provided to sequential_chain.invoke() has the key that the SimpleSequentialChain is expecting. Based on the error message, this expected key is 'input'."
      ],
      "metadata": {
        "id": "CEkVJSdMwjaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_data = {\"input\": \"What is the capital of Bulgaria?\"}\n",
        "response = sequential_chain.invoke(input_data)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wv_egabKvsST",
        "outputId": "0b0ab9a5-acc1-4040-a5da-5b64c05ad3ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mThe capital of Bulgaria is Sofia.\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3mSofia, the capital of Bulgaria, is one of the oldest cities in Europe, with a history that dates back over 2,500 years. It was originally founded as a Thracian settlement called Serdica. An interesting fact about Sofia is that it is home to the Alexander Nevsky Cathedral, one of the largest Eastern Orthodox cathedrals in the world. This stunning architectural masterpiece, completed in 1912, features a striking gold-plated dome and is a symbol of Bulgarian national identity. The cathedral is not only a place of worship but also a significant cultural landmark, attracting visitors with its beautiful mosaics and impressive interior.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{'input': 'What is the capital of Bulgaria?', 'output': 'Sofia, the capital of Bulgaria, is one of the oldest cities in Europe, with a history that dates back over 2,500 years. It was originally founded as a Thracian settlement called Serdica. An interesting fact about Sofia is that it is home to the Alexander Nevsky Cathedral, one of the largest Eastern Orthodox cathedrals in the world. This stunning architectural masterpiece, completed in 1912, features a striking gold-plated dome and is a symbol of Bulgarian national identity. The cathedral is not only a place of worship but also a significant cultural landmark, attracting visitors with its beautiful mosaics and impressive interior.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SequentialChain"
      ],
      "metadata": {
        "id": "cbx-7WCXxBKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain, SequentialChain\n",
        "\n",
        "# Define the first chain: find the capital\n",
        "template1 = \"\"\"You are a helpful assistant.\n",
        "Answer the following question: {question}\n",
        "\"\"\"\n",
        "prompt1 = ChatPromptTemplate.from_template(template1)\n",
        "chain1 = LLMChain(llm=model, prompt=prompt1, output_key=\"capital_city\")\n",
        "\n",
        "# Define the second chain: tell something interesting about the capital\n",
        "template2 = \"\"\"Given the capital city of a country, tell me something interesting about that city.\n",
        "Capital city: {capital_city}\n",
        "\"\"\"\n",
        "prompt2 = ChatPromptTemplate.from_template(template2)\n",
        "chain2 = LLMChain(llm=model, prompt=prompt2, output_key=\"interesting_fact\")\n",
        "\n",
        "# Create the SequentialChain\n",
        "# The input to the SequentialChain is the input to the first chain (question)\n",
        "# The output of the first chain (capital_city) becomes an input to the second chain\n",
        "# The final output of the SequentialChain includes inputs and outputs of the chains\n",
        "sequential_chain = SequentialChain(\n",
        "    chains=[chain1, chain2],\n",
        "    input_variables=[\"question\"],\n",
        "    output_variables=[\"capital_city\", \"interesting_fact\"],\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Run the sequential chain\n",
        "input_data = {\"question\": \"What is the capital of Bulgaria?\"}\n",
        "response = sequential_chain.invoke(input_data)\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2fkIHyuxlZ5",
        "outputId": "220f76fc-5980-4b7a-f98f-89ad7c14d0a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'What is the capital of Bulgaria?',\n",
              " 'capital_city': 'The capital of Bulgaria is Sofia.',\n",
              " 'interesting_fact': 'Sofia, the capital of Bulgaria, is one of the oldest cities in Europe, with a history that spans over 2,000 years. It has been inhabited since at least the 8th century BC and has been known by various names throughout its history, including Serdica and Sredets. One interesting fact about Sofia is that it is located at the foot of Vitosha Mountain, which is a popular destination for hiking and skiing. The mountain is also home to the Vitosha Nature Park, offering a beautiful natural escape just a short distance from the urban environment. Additionally, Sofia is known for its rich cultural heritage, featuring a mix of Roman, Byzantine, and Ottoman influences, which can be seen in its architecture and numerous historical sites.'}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RouterChain"
      ],
      "metadata": {
        "id": "a435daOk6kjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "chain = (\n",
        "    PromptTemplate.from_template(\n",
        "        \"\"\"Given the user question below, classify it as either being about `LangChain`, `OpenAI`, or `Other`.\n",
        "Do not respond with more than one word.\n",
        "\n",
        "<question>\n",
        "{question}\n",
        "</question>\n",
        "\n",
        "Classification:\"\"\"\n",
        "    )\n",
        "    | ChatOpenAI(model_name=\"gpt-4o-mini\",temperature=0)\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "chain.invoke({\"question\": \"how do I call GPT model?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "3SDhZ6UH01yi",
        "outputId": "e135da93-7f37-4da6-d557-2974c881d3bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'OpenAI'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "langchain_chain = PromptTemplate.from_template(\n",
        "    \"\"\"You are an expert in langchain. \\\n",
        "Always answer questions starting with \"As Harrison Chase told me\". \\\n",
        "Respond to the following question:\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        ") | ChatOpenAI(model_name=\"gpt-4o-mini\",temperature=0)\n",
        "\n",
        "openai_chain = PromptTemplate.from_template(\n",
        "    \"\"\"You are an expert in anthropic. \\\n",
        "Always answer questions starting with \"As Dario Amodei told me\". \\\n",
        "Respond to the following question:\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        ") | ChatOpenAI(model_name=\"gpt-4o-mini\",temperature=0)\n",
        "\n",
        "\n",
        "general_chain = PromptTemplate.from_template(\n",
        "    \"\"\"Respond to the following question:\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        ") | ChatOpenAI(model_name=\"gpt-4o-mini\",temperature=0)"
      ],
      "metadata": {
        "id": "GR-NZEx123qD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line creates the full_chain. Let's break down what's happening here:\n",
        "\n",
        "{\"topic\": chain, \"question\": lambda x: x[\"question\"]}: This part creates a dictionary of runnables.\n",
        "\"topic\": chain: This maps the key \"topic\" to the chain object we defined previously. Recall that chain is responsible for classifying the input question into a topic (\"LangChain\", \"OpenAI\", or \"Other\"). When this part of the chain is executed, it will run the chain and the output will be associated with the key \"topic\".\n",
        "\"question\": lambda x: x[\"question\"]: This maps the key \"question\" to a lambda function. A lambda function is a small, anonymous function. In this case, it takes an input x (which will be the input dictionary passed to full_chain.invoke()) and returns the value associated with the key \"question\" from that input dictionary. This essentially just passes the original question through, making it available for later steps.\n",
        "|: This is the pipe operator in LangChain, used to sequentially compose runnables. The output of the left side (the dictionary of runnables) is passed as input to the right side.\n",
        "RunnableLambda(route): This wraps our route function within a RunnableLambda. The route function takes an input dictionary and determines which chain (anthropic_chain, langchain_chain, or general_chain) to execute next based on the \"topic\" key in the input dictionary. The RunnableLambda makes this function executable within the LangChain framework.\n",
        "In essence, the full_chain first runs the chain to classify the input question's topic and simultaneously keeps the original question. Then, it uses the route function to select the appropriate downstream chain based on the classified topic."
      ],
      "metadata": {
        "id": "Fn2W921P9pFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def route(info):\n",
        "    if \"anthropic\" in info[\"topic\"].lower():\n",
        "        return anthropic_chain\n",
        "    elif \"langchain\" in info[\"topic\"].lower():\n",
        "        return langchain_chain\n",
        "    else:\n",
        "        return general_chain"
      ],
      "metadata": {
        "id": "aoDaeAAN6LK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "full_chain = {\"topic\": chain, \"question\": lambda x: x[\"question\"]} | RunnableLambda(\n",
        "    route\n",
        ")\n",
        "full_chain.invoke({\"question\": \"how do I use Anthropic?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zi8OlCMT6Oo8",
        "outputId": "805012ea-6d73-4ed8-cbd5-10a272922c9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='To use Anthropic, you typically need to follow these steps:\\n\\n1. **Access the Platform**: Visit the Anthropic website or the specific platform where their AI models are hosted. You may need to create an account if you don’t have one.\\n\\n2. **Choose a Model**: Anthropic offers various AI models, such as Claude. Select the model that best fits your needs.\\n\\n3. **API Integration**: If you’re a developer, you can integrate Anthropic’s models into your applications using their API. Refer to the API documentation for details on how to authenticate and make requests.\\n\\n4. **Input Your Queries**: Whether using the web interface or API, input your queries or prompts. Be clear and specific to get the best responses from the model.\\n\\n5. **Review and Iterate**: Analyze the responses you receive. You may need to refine your prompts or queries to get more accurate or relevant results.\\n\\n6. **Follow Guidelines**: Ensure you adhere to any usage guidelines or ethical considerations provided by Anthropic to promote responsible AI use.\\n\\n7. **Stay Updated**: Keep an eye on any updates or new features released by Anthropic to enhance your experience.\\n\\nFor specific use cases or advanced features, refer to the official documentation or support resources provided by Anthropic.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 261, 'prompt_tokens': 24, 'total_tokens': 285, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_dbaca60df0', 'id': 'chatcmpl-BVgqU3N3V1hrC5ak8Ujixo4XSnJEP', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--15640db5-4a2e-4bd1-a440-7ba04e736013-0', usage_metadata={'input_tokens': 24, 'output_tokens': 261, 'total_tokens': 285, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "full_chain = {\"topic\": chain, \"question\": lambda x: x[\"question\"]} | RunnableLambda(route) \\\n",
        "full_chain.invoke({\"question\": \"how do I use Anthropic?\"}) \\\n",
        "This line executes the full_chain with the input {\"question\": \"how do I use Anthropic?\"}. The process will be:\n",
        "\n",
        "1. The input {\"question\": \"how do I use Anthropic?\"} is passed to the first part of the chain {\"topic\": chain, \"question\": lambda x: x[\"question\"]}.\n",
        "2. The chain is executed with {\"question\": \"how do I use Anthropic?\"} as input, likely classifying it as \"OpenAI\" (since Anthropic is often associated with the same domain as OpenAI in these examples). The output of chain becomes the value for the \"topic\" key.\n",
        "3. The lambda x: x[\"question\"] is executed with the input {\"question\": \"how do I use Anthropic?\"}, and the value \"how do I use Anthropic?\" becomes the value for the \"question\" key.\n",
        "4. The output of this first step is a dictionary like {\"topic\": \"OpenAI\", \"question\": \"how do I use Anthropic?\"}. This dictionary is passed as input to the RunnableLambda(route).\n",
        "5. The route function is called with this dictionary. It checks the value of \"topic\" (\"OpenAI\"). Based on the route function's logic (specifically the elif \"langchain\" in info[\"topic\"].lower(): return langchain_chain and the else: return general_chain parts, assuming \"OpenAI\" doesn't exactly match \"anthropic\" or \"langchain\"), it will likely select the general_chain.\n",
        "6. The selected chain (general_chain in this case) is then executed with the original question \"how do I use Anthropic?\" as input.\n",
        "7. The final response is the output of the chain selected by the router."
      ],
      "metadata": {
        "id": "1gSjnRhd9xFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_chain.invoke({\"question\": \"how do I use LangChain?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0tPOVoN6Q9F",
        "outputId": "7bf15919-4317-4acd-cf91-21c486ebbdaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='As Harrison Chase told me, to use LangChain, you should start by installing the library via pip. You can do this by running `pip install langchain`. Once installed, you can begin by importing the necessary modules and components based on your use case, such as language models, chains, or agents.\\n\\nNext, you can create a language model instance, configure it with your desired parameters, and then build a chain or agent that utilizes this model. LangChain provides various components to help you structure your application, including prompt templates, memory, and tools for integrating with external APIs or databases.\\n\\nFinally, you can run your chain or agent to process inputs and generate outputs, allowing you to leverage the power of language models in your applications. Be sure to check the official documentation for detailed examples and best practices to get the most out of LangChain.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 171, 'prompt_tokens': 44, 'total_tokens': 215, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_129a36352a', 'id': 'chatcmpl-BVgqlR97yj1V3Ynp36wqTC53X0TDB', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--fb5a0298-454c-4c10-a4da-b5b82a394317-0', usage_metadata={'input_tokens': 44, 'output_tokens': 171, 'total_tokens': 215, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_chain.invoke({\"question\": \"whats 2 + 2\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJOUhFw76W4E",
        "outputId": "c9eae5ad-983b-4abb-84e4-f8584dd17be6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='2 + 2 equals 4.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 24, 'total_tokens': 33, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_dbaca60df0', 'id': 'chatcmpl-BVgqwk4aV0rNR1LY68uJ0gbgmwz4a', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--60c35e96-01a7-4d29-95ef-81ef5c7db1d5-0', usage_metadata={'input_tokens': 24, 'output_tokens': 9, 'total_tokens': 33, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1rdE2HJl6Zel"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}